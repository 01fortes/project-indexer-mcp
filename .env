# Provider Configuration (NEW!)
LLM_PROVIDER=huggingface
LLM_MODEL=moonshotai/Kimi-K2.5:novita
LLM_API_KEY=xxx
# LLM_BASE_URL=http://localhost:1234/v1  # Для локальных моделей

# Reasoning effort для reasoning моделей (low/medium/high)
# Используется только для o1/o3 моделей и codex с reasoning
LLM_REASONING_EFFORT=medium

EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_API_KEY=xxx
# EMBEDDING_BASE_URL=http://localhost:1234/v1

PROVIDER_MAX_RETRIES=3
PROVIDER_TIMEOUT=60

# ChromaDB Configuration
CHROMA_PERSIST_DIRECTORY=/path/to/folder

# Indexing Configuration
MAX_FILE_SIZE_MB=1
MAX_CHUNK_SIZE_TOKENS=6000
CHUNK_OVERLAP_TOKENS=500
MAX_CONCURRENT_FILES=300
MAX_CONCURRENT_AST_PARSING=300
MAX_CONCURRENT_LLM_BATCHES=300
RATE_LIMIT_RPM=3500
RATE_LIMIT_TPM=1000000

# Server Configuration
LOG_LEVEL=DEBUG
SERVER_NAME=project-indexer
SERVER_VERSION=1.0.0

# Web Server Configuration (Admin Portal)
WEB_HOST=0.0.0.0
WEB_PORT=8080
WEB_ENABLE=false
