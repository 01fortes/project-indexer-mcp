# ============================================================================
# Project Scanner MCP - Environment Configuration
# ============================================================================
# Copy this file to .env and fill in your values

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Supported providers: openai, huggingface
LLM_PROVIDER=openai

# ──────────────────────────────────────────────────────────────────────────
# API Keys - Priority order:
# 1. LLM_API_KEY (универсальный, работает для любого провайдера)
# 2. Provider-specific keys (OPENAI_API_KEY, HUGGINGFACE_TOKEN)
# ──────────────────────────────────────────────────────────────────────────

# Универсальный ключ (рекомендуется)
# Работает для любого провайдера, указанного в LLM_PROVIDER
LLM_API_KEY=your_api_key_here

# ИЛИ используйте provider-specific ключи:
# OPENAI_API_KEY=your_openai_key_here
# HUGGINGFACE_TOKEN=your_hf_token_here

# ──────────────────────────────────────────────────────────────────────────
# Models
# ──────────────────────────────────────────────────────────────────────────

# Модель для анализа кода
LLM_MODEL=gpt-5.2-codex

# OpenAI models:
# - gpt-5.2-codex (лучшая для кода)
# - gpt-4o (универсальная)

# HuggingFace models:
# - meta-llama/Llama-3.1-70B-Instruct
# - mistralai/Mixtral-8x7B-Instruct-v0.1
# - codellama/CodeLlama-70b-Instruct-hf
# - Qwen/Qwen2.5-Coder-32B-Instruct

# Reasoning effort (low, medium, high) - только для OpenAI
LLM_REASONING_EFFORT=medium

# Custom endpoint (опционально, для локальных моделей)
# LLM_BASE_URL=http://localhost:8000/v1

# ============================================================================
# Embedding Provider Configuration
# ============================================================================
# Supported providers: openai, huggingface
EMBEDDING_PROVIDER=openai

# ──────────────────────────────────────────────────────────────────────────
# API Keys - Priority order:
# 1. EMBEDDING_API_KEY (универсальный)
# 2. LLM_API_KEY (если тот же провайдер)
# 3. Provider-specific keys
# ──────────────────────────────────────────────────────────────────────────

# Универсальный ключ для embeddings (опционально)
# Если не указан, используется LLM_API_KEY (если провайдер тот же)
# EMBEDDING_API_KEY=your_embedding_key_here

# ──────────────────────────────────────────────────────────────────────────
# Models
# ──────────────────────────────────────────────────────────────────────────

EMBEDDING_MODEL=text-embedding-3-small

# OpenAI models:
# - text-embedding-3-small (1536 dim, быстро, рекомендуется)
# - text-embedding-3-large (3072 dim, качественнее)

# HuggingFace models:
# - sentence-transformers/all-MiniLM-L6-v2 (384 dim, быстро)
# - BAAI/bge-base-en-v1.5 (768 dim, качественно)
# - intfloat/e5-base-v2 (768 dim)
# - sentence-transformers/all-mpnet-base-v2 (768 dim)

# Custom endpoint (опционально)
# EMBEDDING_BASE_URL=http://localhost:8001/v1

# ============================================================================
# Provider Common Settings
# ============================================================================
PROVIDER_MAX_RETRIES=3
PROVIDER_TIMEOUT=60

# ============================================================================
# ChromaDB Configuration
# ============================================================================
# Local persistent storage (default)
CHROMA_PERSIST_DIRECTORY=./chroma_data

# Or use remote ChromaDB server
# CHROMA_HOST=localhost
# CHROMA_PORT=8000

# ============================================================================
# Indexing Configuration
# ============================================================================
MAX_FILE_SIZE_MB=1.0
MAX_CHUNK_SIZE_TOKENS=6000
CHUNK_OVERLAP_TOKENS=500

# ──────────────────────────────────────────────────────────────────────────
# Concurrency Settings
# ──────────────────────────────────────────────────────────────────────────
# MAX_CONCURRENT_FILES: For I/O-bound operations (IndexManager)
# Controls how many files are processed in parallel for embedding/LLM calls
MAX_CONCURRENT_FILES=10

# MAX_CONCURRENT_AST_PARSING: For CPU-bound AST parsing (EnhancedIndexManager Pass 1)
# Controls how many files are parsed in parallel using ThreadPoolExecutor
MAX_CONCURRENT_AST_PARSING=10

# MAX_CONCURRENT_LLM_BATCHES: For LLM enrichment batches (EnhancedIndexManager Pass 2)
# Controls how many LLM batches are processed in parallel during semantic enrichment
MAX_CONCURRENT_LLM_BATCHES=5

# Rate limiting (requests/tokens per minute)
RATE_LIMIT_RPM=3500
RATE_LIMIT_TPM=1000000

# ============================================================================
# Server Configuration
# ============================================================================
LOG_LEVEL=INFO
SERVER_NAME=project-indexer
SERVER_VERSION=1.0.0

# ============================================================================
# Usage Examples
# ============================================================================

# ──────────────────────────────────────────────────────────────────────────
# Example 1: OpenAI с универсальным ключом (рекомендуется)
# ──────────────────────────────────────────────────────────────────────────
# LLM_PROVIDER=openai
# LLM_API_KEY=sk-your-openai-key
# LLM_MODEL=gpt-5.2-codex
#
# EMBEDDING_PROVIDER=openai
# # EMBEDDING_API_KEY не нужен - используется LLM_API_KEY
# EMBEDDING_MODEL=text-embedding-3-small

# ──────────────────────────────────────────────────────────────────────────
# Example 2: HuggingFace с универсальным ключом
# ──────────────────────────────────────────────────────────────────────────
# LLM_PROVIDER=huggingface
# LLM_API_KEY=hf_your_token_here
# LLM_MODEL=meta-llama/Llama-3.1-70B-Instruct
#
# EMBEDDING_PROVIDER=huggingface
# # EMBEDDING_API_KEY не нужен - используется LLM_API_KEY
# EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# ──────────────────────────────────────────────────────────────────────────
# Example 3: Mixed (OpenAI LLM + HuggingFace Embeddings)
# ──────────────────────────────────────────────────────────────────────────
# LLM_PROVIDER=openai
# LLM_API_KEY=sk-your-openai-key
#
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_API_KEY=hf_your_hf_token  # Разные провайдеры = разные ключи
# EMBEDDING_MODEL=BAAI/bge-base-en-v1.5

# ──────────────────────────────────────────────────────────────────────────
# Example 4: Provider-specific keys (старый способ, тоже работает)
# ──────────────────────────────────────────────────────────────────────────
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-...
#
# EMBEDDING_PROVIDER=huggingface
# HUGGINGFACE_TOKEN=hf_...

# ──────────────────────────────────────────────────────────────────────────
# Example 5: Local deployment
# ──────────────────────────────────────────────────────────────────────────
# LLM_PROVIDER=huggingface
# LLM_API_KEY=dummy  # Может не требоваться для локальных
# LLM_BASE_URL=http://localhost:8080/v1
# LLM_MODEL=local-llama
#
# EMBEDDING_PROVIDER=huggingface
# EMBEDDING_BASE_URL=http://localhost:8081/v1
# EMBEDDING_MODEL=local-embeddings
